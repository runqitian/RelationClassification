{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 97\n",
    "POS_MIN = -100\n",
    "POS_EMBED_LEN = 200\n",
    "base_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22549, 50) 22548\n"
     ]
    }
   ],
   "source": [
    "word_embed = pickle.load(open(base_dir + 'word_embed', 'rb'))\n",
    "word_embed = np.transpose(word_embed, )\n",
    "PAD_ID = word_embed.shape[0]-1\n",
    "relation_count = 19\n",
    "print(word_embed.shape, PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22549, 50)\n"
     ]
    }
   ],
   "source": [
    "print(word_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from tfrecord\n",
    "\n",
    "def pad_fixed_length(words):\n",
    "    words = tf.pad(words,tf.constant([[0, MAX_LEN-words.shape[0]]]), constant_values=PAD_ID)\n",
    "    return words\n",
    "\n",
    "def processing(raw):\n",
    "    features = tf.io.parse_single_example(\n",
    "        raw,\n",
    "        features={\n",
    "            'idxs': tf.io.FixedLenFeature([2], tf.int64),\n",
    "            'label': tf.io.FixedLenFeature([1], tf.int64),\n",
    "            'lexical': tf.io.FixedLenFeature([8], tf.int64),\n",
    "            'words': tf.io.VarLenFeature(tf.int64)\n",
    "        }\n",
    "    )\n",
    "    idxs = tf.cast(features['idxs'], tf.int32)\n",
    "    pos1 = tf.range(0, MAX_LEN, 1, dtype=tf.int32) - idxs[0]\n",
    "    pos2 = tf.range(0, MAX_LEN, 1, dtype=tf.int32) - idxs[1]\n",
    "    pos1 = pos1 - POS_MIN\n",
    "    pos2 = pos2 - POS_MIN\n",
    "    label = tf.squeeze(tf.one_hot(features['label'], depth=relation_count), axis=0)\n",
    "    lexical = tf.cast(features['lexical'], tf.int32)\n",
    "    words = tf.cast(tf.sparse.to_dense(features['words']), tf.int32)\n",
    "    words = tf.py_function(pad_fixed_length, [words], Tout=tf.int32)\n",
    "    return pos1, pos2, label, lexical, words\n",
    "\n",
    "train_ds = tf.data.TFRecordDataset(filenames = [base_dir + 'train.tfrecords']).map(processing).shuffle(2000).batch(128)\n",
    "test_ds = tf.data.TFRecordDataset(filenames = [base_dir + 'test.tfrecords']).map(processing).shuffle(2000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 19])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_ds))[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(keras.Model):\n",
    "    def __init__(self, word_embed, pos_dim, word_dim, filters_num, conv_size, output_dim, drop_rate):\n",
    "        super(Network, self).__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        self.word_dim = word_dim\n",
    "        self.drop_rate = drop_rate\n",
    "        self.word_embed = tf.Variable(word_embed, dtype=tf.float32)\n",
    "        self.pos1_embed = tf.Variable(tf.random.uniform([POS_EMBED_LEN, pos_dim],minval=0,maxval=1), dtype=tf.float32)\n",
    "        self.pos2_embed = tf.Variable(tf.random.uniform([POS_EMBED_LEN, pos_dim],minval=0,maxval=1), dtype=tf.float32)\n",
    "        \n",
    "        self.conv1 = layers.Conv1D(filters_num, conv_size, padding='same', activation=tf.nn.relu, bias_initializer=keras.initializers.constant(0.1))\n",
    "        self.pool1 = layers.MaxPool1D(MAX_LEN, padding='same')\n",
    "        self.conv2 = layers.Conv1D(filters_num, 4, padding='same', activation=tf.nn.relu, bias_initializer=keras.initializers.constant(0.1))\n",
    "        self.pool2 = layers.MaxPool1D(MAX_LEN, padding='same')\n",
    "        self.conv3 = layers.Conv1D(filters_num, 5, padding='same', activation=tf.nn.relu, bias_initializer=keras.initializers.constant(0.1))\n",
    "        self.pool3 = layers.MaxPool1D(MAX_LEN, padding='same')\n",
    "        \n",
    "        self.dense1 = layers.Dense(output_dim, kernel_regularizer= keras.regularizers.l2(0.01), bias_initializer=keras.initializers.constant(0.1))\n",
    "        \n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         super(Network, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        pos1 = inputs[0]\n",
    "        pos2 = inputs[1]\n",
    "        lexical = inputs[2]\n",
    "        words = inputs[3]\n",
    "        pf1 = tf.nn.embedding_lookup(self.pos1_embed, pos1)\n",
    "        pf2 = tf.nn.embedding_lookup(self.pos2_embed, pos2)\n",
    "        wf = tf.nn.embedding_lookup(self.word_embed, words)\n",
    "        lexical = tf.nn.embedding_lookup(self.word_embed, lexical)\n",
    "        lexical = tf.reshape(lexical, [-1, 8 * self.word_dim])\n",
    "        wf = tf.concat([pf1, pf2, wf], axis=2)\n",
    "        if training:\n",
    "            wf = tf.nn.dropout(wf, self.drop_rate)\n",
    "        wf1 = self.conv1(wf)\n",
    "        wf1 = self.pool1(wf1)\n",
    "        wf2 = self.conv1(wf)\n",
    "        wf2 = self.pool1(wf2)\n",
    "        wf3 = self.conv1(wf)\n",
    "        wf3 = self.pool1(wf3)\n",
    "        wf = tf.concat([wf1, wf2, wf3], axis=2)\n",
    "        sentence = tf.squeeze(wf, axis=1)\n",
    "        sentence = tf.concat([lexical, sentence], axis=1)\n",
    "        if training:\n",
    "            sentence = tf.nn.dropout(sentence, self.drop_rate)\n",
    "        output = self.dense1(sentence)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(word_embed, 5, 50, 100, 3, relation_count, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "criterion = keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_param = 0.1\n",
    "\n",
    "def train_func(dataset):\n",
    "    global best_acc\n",
    "    loss_count = 0\n",
    "    correct_count = 0\n",
    "    step_count = 0\n",
    "    sample_count = 0\n",
    "    for step, (pos1, pos2, label, lexical, words) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = network([pos1, pos2, lexical, words], training=True)\n",
    "#             print(output.shape, label.shape)\n",
    "            loss = criterion(label, output)\n",
    "            loss += tf.reduce_sum(network.losses)\n",
    "        grads = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "#         print(tf.argmax(output, axis=1).shape, tf.argmax(label, axis=1).shape)\n",
    "        correct = tf.reduce_sum(tf.cast(tf.argmax(output, axis=1) == tf.argmax(label, axis=1), dtype=tf.int32))\n",
    "        \n",
    "        sample_count += label.shape[0]\n",
    "        step_count += 1\n",
    "        correct_count += correct \n",
    "        loss_count += loss\n",
    "    accuracy = correct_count / sample_count\n",
    "    avg_loss = loss_count / step_count\n",
    "    return avg_loss.numpy(), accuracy.numpy(), network.losses\n",
    "\n",
    "def evaluate_func(dataset):\n",
    "    loss_count = 0\n",
    "    correct_count = 0\n",
    "    step_count = 0\n",
    "    sample_count = 0\n",
    "    for step, (pos1, pos2, label, lexical, words) in enumerate(dataset):\n",
    "        output = network([pos1, pos2, lexical, words], training=False)\n",
    "        loss = criterion(label, output)\n",
    "        loss += tf.reduce_sum(network.losses)\n",
    "        correct = tf.reduce_sum(tf.cast(tf.argmax(output, axis=1) == tf.argmax(label, axis=1), dtype=tf.int32))\n",
    "        sample_count += label.shape[0]\n",
    "        step_count += 1\n",
    "        correct_count += correct \n",
    "        loss_count += loss\n",
    "    accuracy = correct_count / sample_count\n",
    "    avg_loss = loss_count / step_count\n",
    "    return avg_loss.numpy(), accuracy.numpy()\n",
    "\n",
    "def predict_func(dataset):\n",
    "    correct_count = 0\n",
    "    sample_count = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for step, (pos1, pos2, label, lexical, words) in enumerate(dataset):\n",
    "        output = network([pos1, pos2, lexical, words], training=False)\n",
    "        pred_batch = tf.argmax(output, axis=1)\n",
    "        lb_batch = tf.argmax(label, axis=1)\n",
    "        correct = tf.reduce_sum(tf.cast(pred_batch == lb_batch, dtype=tf.int32))\n",
    "        sample_count += label.shape[0]\n",
    "        correct_count += correct\n",
    "        y_pred.extend(pred_batch)\n",
    "        y_true.extend(lb_batch)\n",
    "    accuracy = correct_count / sample_count\n",
    "    \n",
    "    return y_pred, y_true, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is 0.3559, accuracy is 0.9415 test loss is 0.9231, accuracy is 0.7722\n",
      "training loss is 0.3443, accuracy is 0.9466 test loss is 0.9248, accuracy is 0.7725\n",
      "training loss is 0.3438, accuracy is 0.9395 test loss is 0.9207, accuracy is 0.7667\n",
      "training loss is 0.3456, accuracy is 0.9433 test loss is 0.9131, accuracy is 0.7681\n",
      "training loss is 0.3341, accuracy is 0.9456 test loss is 0.9342, accuracy is 0.7674\n",
      "training loss is 0.3298, accuracy is 0.9426 test loss is 0.9367, accuracy is 0.7593\n",
      "training loss is 0.3218, accuracy is 0.9475 test loss is 0.9219, accuracy is 0.7637\n",
      "training loss is 0.3194, accuracy is 0.9496 test loss is 0.9321, accuracy is 0.7637\n",
      "training loss is 0.3015, accuracy is 0.9543 test loss is 0.9369, accuracy is 0.7667\n",
      "training loss is 0.3048, accuracy is 0.9481 test loss is 0.9362, accuracy is 0.7689\n",
      "best accuracy is 0.7725432462274567\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "save_dir = './saved/'\n",
    "for epoch in range(80):\n",
    "    train_avg_loss, train_accuracy, reg_loss = train_func(train_ds)\n",
    "    test_avg_loss, test_accuracy = evaluate_func(test_ds)\n",
    "    if test_accuracy > best_acc:\n",
    "        network.save_weights(save_dir + 'bestckpt')\n",
    "        best_acc = test_accuracy\n",
    "#     print(reg_loss)\n",
    "    print('training loss is {0:.4f}, accuracy is {1:.4f} test loss is {2:.4f}, accuracy is {3:.4f}'.format(train_avg_loss,\\\n",
    "                                                            train_accuracy, test_avg_loss, test_accuracy))\n",
    "print('best accuracy is {}'.format(best_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7725432462274567\n"
     ]
    }
   ],
   "source": [
    "network.load_weights(save_dir + 'bestckpt19')\n",
    "y_true, y_pred, accuracy = predict_func(test_ds)\n",
    "y_true = [item.numpy() for item in y_true]\n",
    "y_pred = [item.numpy() for item in y_pred]\n",
    "print(accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "id2rel = pickle.load(open(base_dir + 'lb2rel.dict', 'rb'))\n",
    "unique_relations = pickle.load(open(base_dir + 'unique_relations', 'rb'))\n",
    "y_true = [id2rel[item] for item in y_true]\n",
    "y_pred = [id2rel[item] for item in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Member-Collection', 'Message-Topic', 'Instrument-Agency', 'Entity-Origin', 'Other', 'Content-Container', 'Component-Whole', 'Product-Producer', 'Entity-Destination', 'Cause-Effect']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ptn = re.compile(r'\\(e1,e2\\)|\\(e2,e1\\)')\n",
    "unique_relations = list(set([re.sub(ptn, '', item) for item in unique_relations]))\n",
    "print(unique_relations)\n",
    "y_true = [re.sub(ptn, '', item) for item in y_true]\n",
    "y_pred = [re.sub(ptn, '', item) for item in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Member-Collection       0.85      0.82      0.84       242\n",
      "     Message-Topic       0.89      0.79      0.83       294\n",
      " Instrument-Agency       0.71      0.70      0.71       158\n",
      "     Entity-Origin       0.84      0.83      0.83       262\n",
      " Content-Container       0.84      0.81      0.82       201\n",
      "   Component-Whole       0.80      0.79      0.80       316\n",
      "  Product-Producer       0.75      0.81      0.78       215\n",
      "Entity-Destination       0.91      0.84      0.87       318\n",
      "      Cause-Effect       0.93      0.90      0.91       340\n",
      "\n",
      "         micro avg       0.85      0.82      0.83      2346\n",
      "         macro avg       0.84      0.81      0.82      2346\n",
      "      weighted avg       0.85      0.82      0.83      2346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, labels = [item for item in unique_relations if item !='Other']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
