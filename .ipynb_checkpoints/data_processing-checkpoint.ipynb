{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import re\n",
    "e1pt = re.compile('<e1>.*</e1>')\n",
    "e2pt = re.compile('<e2>.*</e2>')\n",
    "E1LABEL = 'E1LABEL'\n",
    "E2LABEL = 'E2LABEL'\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "    # sentence\n",
    "    # entity_idx\n",
    "    # context_idx\n",
    "    # hypernyms\n",
    "    # words\n",
    "    # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 1193514\n",
      "finished 1190000\n",
      "1193514\n"
     ]
    }
   ],
   "source": [
    "if WORD_DIM == 50:\n",
    "    with open('./glove.twitter.27B.50d.txt', 'r') as rf:\n",
    "        word_dict = {}\n",
    "        lines = rf.readlines()\n",
    "        count = 0\n",
    "        print('length is {}'.format(len(lines)))\n",
    "        for line in lines:\n",
    "            ls = line[:-1].split(' ')\n",
    "            word_dict[ls[0]] = [float(item) for item in ls[1:]]\n",
    "            if count % 10000 == 0:\n",
    "                print('\\rfinished {}'.format(count), end='', flush=True)\n",
    "            count += 1\n",
    "    #         break\n",
    "        print('')\n",
    "    print(len(word_dict))\n",
    "else:\n",
    "    print('word_dim not right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 8000\n",
      "2717 2717\n"
     ]
    }
   ],
   "source": [
    "# read text file\n",
    "def read_txt(path):\n",
    "    with open(path, 'r') as rf:\n",
    "        lines = rf.readlines()\n",
    "        li = 0\n",
    "        sentences = []\n",
    "        relations = []\n",
    "        for i in range(0,len(lines), 4):\n",
    "            sentences.append(lines[i])\n",
    "            relations.append(lines[i+1])\n",
    "    print(len(sentences), len(relations))\n",
    "    return sentences, relations\n",
    "\n",
    "train_sentences, train_relations = read_txt('./raw/TRAIN_FILE.TXT')\n",
    "test_sentences, test_relations = read_txt('./raw/TEST_FILE_FULL.TXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 ['Component-Whole(e2,e1)', 'Other', 'Instrument-Agency(e2,e1)']\n",
      "2717 ['Message-Topic(e1,e2)', 'Product-Producer(e2,e1)', 'Instrument-Agency(e2,e1)']\n",
      "19 ['Entity-Destination(e1,e2)', 'Message-Topic(e1,e2)', 'Cause-Effect(e1,e2)', 'Content-Container(e2,e1)', 'Instrument-Agency(e1,e2)', 'Entity-Origin(e1,e2)', 'Content-Container(e1,e2)', 'Component-Whole(e2,e1)', 'Other', 'Product-Producer(e2,e1)', 'Entity-Origin(e2,e1)', 'Member-Collection(e1,e2)', 'Product-Producer(e1,e2)', 'Instrument-Agency(e2,e1)', 'Cause-Effect(e2,e1)', 'Message-Topic(e2,e1)', 'Entity-Destination(e2,e1)', 'Component-Whole(e1,e2)', 'Member-Collection(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "# process labels\n",
    "# for i in range(len(train_relations)):\n",
    "#     if train_relations[i] == 'Other\\n':\n",
    "#         train_relations[i] = train_relations[i][:-1]\n",
    "#         continue\n",
    "#     train_relations[i] = train_relations[i][:-8]\n",
    "# for i in range(len(test_relations)):\n",
    "#     if test_relations[i] == 'Other\\n':\n",
    "#         test_relations[i] = test_relations[i][:-1]\n",
    "#         continue\n",
    "#     test_relations[i] = test_relations[i][:-8]\n",
    "\n",
    "for i in range(len(train_relations)):\n",
    "    train_relations[i] = train_relations[i][:-1]\n",
    "for i in range(len(test_relations)):\n",
    "    test_relations[i] = test_relations[i][:-1]\n",
    "    \n",
    "\n",
    "unique_relations = list(set(test_relations + train_relations))\n",
    "print(len(train_relations), train_relations[:3])\n",
    "print(len(test_relations), test_relations[:3])\n",
    "print(len(unique_relations), unique_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 2717 [1, 9, 13]\n"
     ]
    }
   ],
   "source": [
    "# unique_relations = ['Message-Topic', 'Instrument-Agency', 'Product-Producer', 'Content-Container', 'Entity-Origin', 'Component-Whole', 'Entity-Destination', 'Member-Collection', 'Cause-Effect',  'Other']\n",
    "rel2lb = {}\n",
    "lb2rel = {}\n",
    "for i,item in enumerate(unique_relations):\n",
    "    rel2lb[item] = i\n",
    "    lb2rel[i] = item\n",
    "train_labels = [rel2lb[item] for item in train_relations]\n",
    "test_labels = [rel2lb[item] for item in test_relations]\n",
    "print(len(train_labels), len(test_labels), test_labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence):\n",
    "    # regex replace\n",
    "    sentence = re.sub('\\d*\\t\\\"', \"\", sentence)\n",
    "    sentence = sentence[:-2]\n",
    "    e1 = e1pt.search(sentence)\n",
    "    sentence = re.sub(e1pt, E1LABEL, sentence)\n",
    "    e2 = e2pt.search(sentence)\n",
    "    sentence = re.sub(e2pt, E2LABEL, sentence)\n",
    "    \n",
    "    # split word\n",
    "    words= []\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        words.append(token.text)\n",
    "    \n",
    "    return sentence, (e1.group(0)[4:-5],e2.group(0)[4:-5]), words\n",
    "\n",
    "def process_sentence_group(org_sentences):\n",
    "    entities = []\n",
    "    words_list = []\n",
    "    processed_sentences = []\n",
    "    for i in range(len(org_sentences)):\n",
    "        sentence = org_sentences[i]\n",
    "        sentence, entity, words =  process_sentence(sentence)\n",
    "        entities.append(entity)\n",
    "        words_list.append(words)\n",
    "        processed_sentences.append(sentence)\n",
    "        if (i%100 == 0):\n",
    "            print('\\rfinished {}'.format(i),flush=True, end='')\n",
    "    print('')\n",
    "    return processed_sentences, entities, words_list\n",
    "\n",
    "train_processed, train_entities, train_words = process_sentence_group(train_sentences)\n",
    "test_processed, test_entities, test_words = process_sentence_group(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The system as described above has its greatest application in an arrayed E1LABEL of antenna E2LABEL.', 'The E1LABEL was carefully wrapped and bound into the E2LABEL by means of a cord.', 'The E1LABEL of a keygen uses a E2LABEL to look at the raw assembly code.']\n",
      "[('configuration', 'elements'), ('child', 'cradle'), ('author', 'disassembler')]\n",
      "[['The', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'E1LABEL', 'of', 'antenna', 'E2LABEL', '.'], ['The', 'E1LABEL', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', 'E2LABEL', 'by', 'means', 'of', 'a', 'cord', '.'], ['The', 'E1LABEL', 'of', 'a', 'keygen', 'uses', 'a', 'E2LABEL', 'to', 'look', 'at', 'the', 'raw', 'assembly', 'code', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(train_processed[:3])\n",
    "print(train_entities[:3])\n",
    "print(train_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "def lexical_features(org_words):\n",
    "    context_words = []\n",
    "    word_idxs = []\n",
    "    count = 0\n",
    "    for each in org_words:\n",
    "        w1 = ''\n",
    "        w2 = ''\n",
    "        w3 = ''\n",
    "        w4 = ''\n",
    "        idx1 = 0\n",
    "        idx2 = 0\n",
    "        for i in range(len(each)):\n",
    "            if each[i] == E1LABEL:\n",
    "                w1 = each[i-1] if i-1>=0 else ''\n",
    "                w2 = each[i+1] if i+1<len(each) else ''\n",
    "                idx1 = i\n",
    "            if each[i] == E2LABEL:\n",
    "                w3 = each[i-1] if i-1>=0 else ''\n",
    "                w4 = each[i+1] if i+1<len(each) else ''\n",
    "                idx2 = i\n",
    "        context_words.append((w1, w2, w3, w4))\n",
    "        word_idxs.append((idx1, idx2))\n",
    "        if count%100 == 0:\n",
    "            print('\\rfinished {}'.format(count), flush=True, end='')\n",
    "        count += 1\n",
    "    print('')\n",
    "    return context_words, word_idxs\n",
    "\n",
    "train_context,train_idxs = lexical_features(train_words)\n",
    "test_context,test_idxs = lexical_features(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arrayed', 'of', 'antenna', '.'), ('The', 'was', 'the', 'by'), ('The', 'of', 'a', 'to')]\n",
      "[(12, 15), (1, 9), (1, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(train_context[:3])\n",
    "print(train_idxs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n",
      "[('configuration', 'elements'), ('child', 'cradle'), ('author', 'disassembler')]\n",
      "[['the', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'configuration', 'of', 'antenna', 'elements', '.'], ['the', 'child', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', 'cradle', 'by', 'means', 'of', 'a', 'cord', '.'], ['the', 'author', 'of', 'a', 'keygen', 'uses', 'a', 'disassembler', 'to', 'look', 'at', 'the', 'raw', 'assembly', 'code', '.']]\n"
     ]
    }
   ],
   "source": [
    "def replace_word(org_list, org_word_list):\n",
    "    new_list = []\n",
    "    for i in range(len(org_list)):\n",
    "        e1 = (nlp(org_list[i][0])[-1].text).lower()\n",
    "        e2 = (nlp(org_list[i][1])[-1].text).lower()\n",
    "        new_list.append((e1,e2))\n",
    "        for j in range(len(org_word_list[i])):\n",
    "            if org_word_list[i][j] == E1LABEL:\n",
    "                org_word_list[i][j] = e1\n",
    "            if org_word_list[i][j] == E2LABEL:\n",
    "                org_word_list[i][j] = e2\n",
    "            org_word_list[i][j] = org_word_list[i][j].lower()\n",
    "        if i%100 == 0:\n",
    "            print('\\rfinished {}'.format(i), flush=True, end='')\n",
    "    print('')\n",
    "    return new_list\n",
    "\n",
    "train_entities = replace_word(train_entities, train_words)\n",
    "test_entities = replace_word(test_entities, test_words)\n",
    "print(train_entities[:3])\n",
    "print(train_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n",
      "[('design', 'weather'), ('juvenile', 'bed'), ('communicator', '')]\n"
     ]
    }
   ],
   "source": [
    "def get_hypernyms(org_entities):\n",
    "    hypernyms = []\n",
    "    for i in range(len(org_entities)):\n",
    "        h1 = ''\n",
    "        h2 = ''\n",
    "        temp = wn.synsets(org_entities[i][0])\n",
    "        if len(temp) > 0:\n",
    "            t2 = temp[0].hypernyms()\n",
    "            if len(t2) > 0:\n",
    "                h1 = t2[0].lemmas()[0].name()\n",
    "        temp = wn.synsets(org_entities[i][1])\n",
    "        if len(temp) > 0:\n",
    "            t2 = temp[0].hypernyms()\n",
    "            if len(t2) > 0:\n",
    "                h2 = t2[0].lemmas()[0].name()\n",
    "        \n",
    "        h1 = h1.split('_')[-1]\n",
    "        h2 = h2.split('_')[-1]\n",
    "        hypernyms.append((h1, h2))\n",
    "        if i%100 == 0:\n",
    "            print('\\rfinished {}'.format(i), flush=True, end='')\n",
    "    print('')\n",
    "    return hypernyms\n",
    "\n",
    "train_hypernyms = get_hypernyms(train_entities)\n",
    "test_hypernyms = get_hypernyms(test_entities)\n",
    "print(train_hypernyms[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([w for item in train_words+test_words for w in item]))\n",
    "unique_words.append('<unknown_word>')\n",
    "unique_words.append('<padding_word>')\n",
    "unknown_idx = len(unique_words) - 2\n",
    "padding_idx = len(unique_words) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22549 22549\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for i,w in enumerate(unique_words):\n",
    "    word2idx[w] = i\n",
    "    idx2word[i] = w\n",
    "print(len(word2idx), len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33719    -0.70544    -0.9214848 ]\n",
      " [-0.21405     0.53588    -0.6224373 ]\n",
      " [-0.75152     0.29921     0.40707925]]\n",
      "3277 ['802.11a', 'ingots', 'karelian'] 22549\n"
     ]
    }
   ],
   "source": [
    "not_in_count = 0\n",
    "not_in_words = []\n",
    "def get_word_embed(unique_words, word_dict):\n",
    "    global not_in_count, not_in_words\n",
    "    word_embed = np.zeros([WORD_DIM, len(unique_words)], dtype= 'float32')\n",
    "    for i, w in enumerate(unique_words):\n",
    "        if w not in word_dict:\n",
    "            not_in_count += 1\n",
    "            not_in_words.append(w)\n",
    "        word_embed[:, i] = word_dict[w] if w in word_dict else np.random.normal(0,1,50)\n",
    "    word_embed[:,-1] = np.zeros(50, dtype='float')\n",
    "    return word_embed\n",
    "word_embed = get_word_embed(unique_words, word_dict)\n",
    "print(word_embed[:3,:3])\n",
    "print(not_in_count, not_in_words[:3],len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 8000\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "    # sentence\n",
    "    # entity_idx\n",
    "    # entity_words\n",
    "    # context_words\n",
    "    # hypernyms\n",
    "    # words\n",
    "    # label\n",
    "train_iter = zip(train_sentences, train_idxs, train_entities, train_context, train_hypernyms, train_words, train_labels)\n",
    "test_iter = zip(test_sentences, test_idxs, test_entities, test_context, test_hypernyms, test_words, test_labels)\n",
    "\n",
    "\n",
    "def transform_and_write_tf(data):\n",
    "    tf_ex_ser = []\n",
    "    for i,each in enumerate(data):\n",
    "        sentence = each[0]\n",
    "        idxs = [item for item in each[1]]\n",
    "        lexical = each[2] + each[3] + each[4]\n",
    "        words = each[5]\n",
    "        label = each[6]\n",
    "        lexical = [word2idx[item] if item in word2idx else unknown_idx for item in lexical]\n",
    "        words = [word2idx[item] if item in word2idx else unknown_idx for item in words]\n",
    "        f1 = tf.train.Feature(bytes_list=tf.train.BytesList(value=[sentence.encode('utf-8')]))\n",
    "        f2 = tf.train.Feature(int64_list=tf.train.Int64List(value=idxs))\n",
    "        f3 = tf.train.Feature(int64_list=tf.train.Int64List(value=lexical))\n",
    "        f4 = tf.train.Feature(int64_list=tf.train.Int64List(value=words))\n",
    "        f5 = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "        feature = {\n",
    "            'sentece': f1,\n",
    "            'idxs': f2,\n",
    "            'lexical': f3,\n",
    "            'words':f4,\n",
    "            'label':f5\n",
    "        }\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "        tf_ex_ser.append(example)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('\\rfinished {}'.format(i+1), end='', flush=True)\n",
    "    print('')\n",
    "    return tf_ex_ser\n",
    "train_tf = transform_and_write_tf(train_iter)\n",
    "test_tf = transform_and_write_tf(test_iter)\n",
    "#     print(sentence)\n",
    "#     print(idxs)\n",
    "#     print(lexical)\n",
    "#     print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "record_file = base_dir + 'train.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for example in train_tf:\n",
    "        writer.write(example)\n",
    "\n",
    "record_file = base_dir + 'test.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for example in test_tf:\n",
    "        writer.write(example)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2idx, open(base_dir + 'word2idx.dict', 'wb'))\n",
    "pickle.dump(idx2word, open(base_dir + 'idx2word.dict', 'wb'))\n",
    "pickle.dump(lb2rel, open(base_dir + 'lb2rel.dict', 'wb'))\n",
    "pickle.dump(rel2lb, open(base_dir + 'rel2lb.dict', 'wb'))\n",
    "pickle.dump(word_embed, open(base_dir + 'word_embed','wb'))\n",
    "pickle.dump(unique_relations, open(base_dir + 'unique_relations', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entity-Destination(e1,e2)', 'Message-Topic(e1,e2)', 'Cause-Effect(e1,e2)', 'Content-Container(e2,e1)', 'Instrument-Agency(e1,e2)', 'Entity-Origin(e1,e2)', 'Content-Container(e1,e2)', 'Component-Whole(e2,e1)', 'Other', 'Product-Producer(e2,e1)', 'Entity-Origin(e2,e1)', 'Member-Collection(e1,e2)', 'Product-Producer(e1,e2)', 'Instrument-Agency(e2,e1)', 'Cause-Effect(e2,e1)', 'Message-Topic(e2,e1)', 'Entity-Destination(e2,e1)', 'Component-Whole(e1,e2)', 'Member-Collection(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
